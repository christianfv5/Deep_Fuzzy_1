{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# Load the MNIST data.\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train -= x_train_mean\n",
    "x_test -= x_train_mean\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The depth of the network can be changed with parameter n.\n",
    "The fuzzy classifier can be changed with the use_anfis parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0413 13:45:09.258332 140026519725888 deprecation.py:506] From /home/ryan-desktop/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow/python/training/moving_averages.py:210: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.004\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from model import LogGaussMF\n",
    "from resnet_backend import *\n",
    "\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', 4. * lr)\n",
    "    return 4. * lr\n",
    "\n",
    "\n",
    "# Prepare model model saving directory.\n",
    "def get_callbacks(model_type):\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    model_name = \"%s_model.{epoch:03d}.h5\" % model_type\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "    # Prepare callbacks for model saving and for learning rate adjustment.\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=filepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True)\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(\n",
    "        factor=np.sqrt(0.1),\n",
    "        cooldown=0,\n",
    "        patience=5,\n",
    "        min_lr=0.5e-6)\n",
    "\n",
    "    return [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "\n",
    "use_anfis = True\n",
    "\n",
    "n = 6\n",
    "depth = n * 9 + 2\n",
    "\n",
    "# Model name, depth and version\n",
    "model_type = 'Fuzzy-ResNet%dv%d' % (depth, 2)\n",
    "\n",
    "inputs, features = resnet_backend_v2(\n",
    "    input_shape=input_shape, \n",
    "    depth=depth)\n",
    "\n",
    "memberships = LogGaussMF(10)(features)\n",
    "rules = Lambda(lambda x: K.sum(x, axis=-1))(memberships)\n",
    "\n",
    "if use_anfis:\n",
    "    linear = Dense(10)(features)\n",
    "    logits = Add()([rules, linear])\n",
    "else:\n",
    "    logits = rules\n",
    "\n",
    "outputs = Activation(\"softmax\")(logits)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(lr=lr_schedule(0)),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "callbacks = get_callbacks(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 58s 125ms/step - loss: 1.0918 - acc: 0.7703 - val_loss: 0.7969 - val_acc: 0.8072\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.80720, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.001.h5\n",
      "Epoch 2/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 46s 99ms/step - loss: 0.6845 - acc: 0.8278 - val_loss: 0.8027 - val_acc: 0.7820\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.80720\n",
      "Epoch 3/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.6006 - acc: 0.8469 - val_loss: 1.4112 - val_acc: 0.5759\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.80720\n",
      "Epoch 4/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.5586 - acc: 0.8546 - val_loss: 0.6523 - val_acc: 0.8215\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.80720 to 0.82150, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.004.h5\n",
      "Epoch 5/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.5288 - acc: 0.8630 - val_loss: 0.9004 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.82150\n",
      "Epoch 6/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.5097 - acc: 0.8678 - val_loss: 1.0561 - val_acc: 0.7284\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82150\n",
      "Epoch 7/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.4898 - acc: 0.8713 - val_loss: 0.7471 - val_acc: 0.7933\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.82150\n",
      "Epoch 8/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.4729 - acc: 0.8755 - val_loss: 1.2984 - val_acc: 0.6558\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.82150\n",
      "Epoch 9/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 46s 99ms/step - loss: 0.4613 - acc: 0.8775 - val_loss: 1.0683 - val_acc: 0.6723\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.82150\n",
      "Epoch 10/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.4561 - acc: 0.8776 - val_loss: 0.7203 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82150\n",
      "Epoch 11/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.4452 - acc: 0.8811 - val_loss: 0.5363 - val_acc: 0.8506\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.82150 to 0.85060, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.011.h5\n",
      "Epoch 12/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.4360 - acc: 0.8839 - val_loss: 0.9698 - val_acc: 0.7609\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.85060\n",
      "Epoch 13/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.4245 - acc: 0.8858 - val_loss: 0.5603 - val_acc: 0.8450\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.85060\n",
      "Epoch 14/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.4235 - acc: 0.8859 - val_loss: 0.5312 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.85060\n",
      "Epoch 15/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.4181 - acc: 0.8870 - val_loss: 0.5535 - val_acc: 0.8447\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.85060\n",
      "Epoch 16/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 46s 99ms/step - loss: 0.4099 - acc: 0.8904 - val_loss: 0.5382 - val_acc: 0.8531\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.85060 to 0.85310, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.016.h5\n",
      "Epoch 17/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 46s 99ms/step - loss: 0.4049 - acc: 0.8911 - val_loss: 0.5102 - val_acc: 0.8624\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.85310 to 0.86240, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.017.h5\n",
      "Epoch 18/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.4036 - acc: 0.8922 - val_loss: 0.8248 - val_acc: 0.7451\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86240\n",
      "Epoch 19/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3944 - acc: 0.8947 - val_loss: 0.4175 - val_acc: 0.8873\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.86240 to 0.88730, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.019.h5\n",
      "Epoch 20/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.3923 - acc: 0.8952 - val_loss: 0.5557 - val_acc: 0.8309\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88730\n",
      "Epoch 21/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3924 - acc: 0.8953 - val_loss: 0.5746 - val_acc: 0.8295\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88730\n",
      "Epoch 22/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.3843 - acc: 0.8966 - val_loss: 0.6344 - val_acc: 0.8093\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88730\n",
      "Epoch 23/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3790 - acc: 0.8982 - val_loss: 0.5410 - val_acc: 0.8452\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.88730\n",
      "Epoch 24/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3837 - acc: 0.8959 - val_loss: 0.4773 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.88730\n",
      "Epoch 25/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 46s 99ms/step - loss: 0.3731 - acc: 0.8990 - val_loss: 0.4050 - val_acc: 0.8902\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.88730 to 0.89020, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.025.h5\n",
      "Epoch 26/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.3733 - acc: 0.8988 - val_loss: 0.7460 - val_acc: 0.7713\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.89020\n",
      "Epoch 27/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 46s 99ms/step - loss: 0.3716 - acc: 0.9005 - val_loss: 0.4656 - val_acc: 0.8683\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.89020\n",
      "Epoch 28/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3657 - acc: 0.9005 - val_loss: 0.4702 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.89020\n",
      "Epoch 29/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 46s 99ms/step - loss: 0.3694 - acc: 0.8999 - val_loss: 0.4311 - val_acc: 0.8815\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.89020\n",
      "Epoch 30/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3665 - acc: 0.9006 - val_loss: 0.6385 - val_acc: 0.8171\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.89020\n",
      "Epoch 31/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3619 - acc: 0.9022 - val_loss: 0.5516 - val_acc: 0.8332\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.89020\n",
      "Epoch 32/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3606 - acc: 0.9023 - val_loss: 0.6694 - val_acc: 0.8025\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.89020\n",
      "Epoch 33/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3580 - acc: 0.9026 - val_loss: 0.4616 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.89020\n",
      "Epoch 34/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3591 - acc: 0.9026 - val_loss: 0.4199 - val_acc: 0.8845\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.89020\n",
      "Epoch 35/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3543 - acc: 0.9047 - val_loss: 0.5435 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.89020\n",
      "Epoch 36/200\n",
      "Learning rate:  0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 44s 94ms/step - loss: 0.3532 - acc: 0.9043 - val_loss: 0.6262 - val_acc: 0.8204\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.89020\n",
      "Epoch 37/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.3524 - acc: 0.9050 - val_loss: 0.4139 - val_acc: 0.8882\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.89020\n",
      "Epoch 38/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.3503 - acc: 0.9040 - val_loss: 0.6698 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.89020\n",
      "Epoch 39/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.3494 - acc: 0.9052 - val_loss: 0.6408 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.89020\n",
      "Epoch 40/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.3480 - acc: 0.9049 - val_loss: 0.5073 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89020\n",
      "Epoch 41/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 101ms/step - loss: 0.3468 - acc: 0.9063 - val_loss: 0.4547 - val_acc: 0.8694\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89020\n",
      "Epoch 42/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3446 - acc: 0.9076 - val_loss: 0.4192 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89020\n",
      "Epoch 43/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3423 - acc: 0.9074 - val_loss: 0.5438 - val_acc: 0.8408\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89020\n",
      "Epoch 44/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3437 - acc: 0.9058 - val_loss: 0.5905 - val_acc: 0.8230\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89020\n",
      "Epoch 45/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3404 - acc: 0.9060 - val_loss: 0.4613 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89020\n",
      "Epoch 46/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3401 - acc: 0.9074 - val_loss: 0.5216 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89020\n",
      "Epoch 47/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 101ms/step - loss: 0.3426 - acc: 0.9058 - val_loss: 0.4832 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89020\n",
      "Epoch 48/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.3379 - acc: 0.9082 - val_loss: 0.5066 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89020\n",
      "Epoch 49/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3401 - acc: 0.9053 - val_loss: 0.4889 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89020\n",
      "Epoch 50/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.3367 - acc: 0.9086 - val_loss: 0.4098 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89020\n",
      "Epoch 51/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3350 - acc: 0.9077 - val_loss: 0.5078 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89020\n",
      "Epoch 52/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.3349 - acc: 0.9083 - val_loss: 0.6344 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89020\n",
      "Epoch 53/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3298 - acc: 0.9095 - val_loss: 0.4175 - val_acc: 0.8776\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89020\n",
      "Epoch 54/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3332 - acc: 0.9088 - val_loss: 0.5969 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89020\n",
      "Epoch 55/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.3312 - acc: 0.9098 - val_loss: 0.5141 - val_acc: 0.8350\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89020\n",
      "Epoch 56/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3306 - acc: 0.9083 - val_loss: 0.4394 - val_acc: 0.8783\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.89020\n",
      "Epoch 57/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3332 - acc: 0.9085 - val_loss: 0.4687 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89020\n",
      "Epoch 58/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3272 - acc: 0.9101 - val_loss: 0.5219 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.89020\n",
      "Epoch 59/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3294 - acc: 0.9092 - val_loss: 0.5065 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.89020\n",
      "Epoch 60/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3275 - acc: 0.9105 - val_loss: 0.4331 - val_acc: 0.8747\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.89020\n",
      "Epoch 61/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3230 - acc: 0.9127 - val_loss: 0.6402 - val_acc: 0.8215\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.89020\n",
      "Epoch 62/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3238 - acc: 0.9112 - val_loss: 0.7901 - val_acc: 0.7726\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.89020\n",
      "Epoch 63/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3246 - acc: 0.9095 - val_loss: 0.4966 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.89020\n",
      "Epoch 64/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3275 - acc: 0.9098 - val_loss: 0.4370 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89020\n",
      "Epoch 65/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.3252 - acc: 0.9104 - val_loss: 0.4571 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89020\n",
      "Epoch 66/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.3220 - acc: 0.9124 - val_loss: 0.4143 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.89020\n",
      "Epoch 67/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3225 - acc: 0.9100 - val_loss: 0.5432 - val_acc: 0.8367\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89020\n",
      "Epoch 68/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3213 - acc: 0.9112 - val_loss: 0.5744 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89020\n",
      "Epoch 69/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3229 - acc: 0.9106 - val_loss: 0.4234 - val_acc: 0.8769\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89020\n",
      "Epoch 70/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3247 - acc: 0.9101 - val_loss: 0.3960 - val_acc: 0.8888\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89020\n",
      "Epoch 71/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3210 - acc: 0.9124 - val_loss: 0.4439 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.89020\n",
      "Epoch 72/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3197 - acc: 0.9121 - val_loss: 0.7510 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89020\n",
      "Epoch 73/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 99ms/step - loss: 0.3206 - acc: 0.9121 - val_loss: 0.4500 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89020\n",
      "Epoch 74/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.3208 - acc: 0.9126 - val_loss: 0.4956 - val_acc: 0.8522\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89020\n",
      "Epoch 75/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.3170 - acc: 0.9136 - val_loss: 0.4257 - val_acc: 0.8726\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89020\n",
      "Epoch 76/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.3170 - acc: 0.9133 - val_loss: 0.4056 - val_acc: 0.8826\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89020\n",
      "Epoch 77/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.3188 - acc: 0.9120 - val_loss: 0.4664 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89020\n",
      "Epoch 78/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.3180 - acc: 0.9124 - val_loss: 0.5558 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89020\n",
      "Epoch 79/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.3191 - acc: 0.9120 - val_loss: 0.3461 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00079: val_acc improved from 0.89020 to 0.90620, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.079.h5\n",
      "Epoch 80/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.3146 - acc: 0.9163 - val_loss: 0.4303 - val_acc: 0.8752\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.90620\n",
      "Epoch 81/200\n",
      "Learning rate:  0.004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.3141 - acc: 0.9131 - val_loss: 0.4715 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.90620\n",
      "Epoch 82/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2748 - acc: 0.9278 - val_loss: 0.2880 - val_acc: 0.9258\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.90620 to 0.92580, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.082.h5\n",
      "Epoch 83/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2639 - acc: 0.9318 - val_loss: 0.2764 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00083: val_acc improved from 0.92580 to 0.92940, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.083.h5\n",
      "Epoch 84/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.2539 - acc: 0.9334 - val_loss: 0.2794 - val_acc: 0.9255\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.92940\n",
      "Epoch 85/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.2484 - acc: 0.9351 - val_loss: 0.2697 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00085: val_acc improved from 0.92940 to 0.93100, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.085.h5\n",
      "Epoch 86/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.2464 - acc: 0.9358 - val_loss: 0.2665 - val_acc: 0.9300\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.93100\n",
      "Epoch 87/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2415 - acc: 0.9373 - val_loss: 0.2631 - val_acc: 0.9302\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.93100\n",
      "Epoch 88/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2419 - acc: 0.9367 - val_loss: 0.2615 - val_acc: 0.9308\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.93100\n",
      "Epoch 89/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.2388 - acc: 0.9357 - val_loss: 0.2640 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00089: val_acc improved from 0.93100 to 0.93140, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.089.h5\n",
      "Epoch 90/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.2333 - acc: 0.9379 - val_loss: 0.2613 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.93140\n",
      "Epoch 91/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2311 - acc: 0.9388 - val_loss: 0.2640 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.93140\n",
      "Epoch 92/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2336 - acc: 0.9358 - val_loss: 0.2571 - val_acc: 0.9301\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.93140\n",
      "Epoch 93/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2247 - acc: 0.9400 - val_loss: 0.2634 - val_acc: 0.9285\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.93140\n",
      "Epoch 94/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2273 - acc: 0.9390 - val_loss: 0.2595 - val_acc: 0.9300\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.93140\n",
      "Epoch 95/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.2231 - acc: 0.9399 - val_loss: 0.2528 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00095: val_acc improved from 0.93140 to 0.93210, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.095.h5\n",
      "Epoch 96/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2218 - acc: 0.9396 - val_loss: 0.2707 - val_acc: 0.9235\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.93210\n",
      "Epoch 97/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2214 - acc: 0.9405 - val_loss: 0.2486 - val_acc: 0.9318\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.93210\n",
      "Epoch 98/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2207 - acc: 0.9393 - val_loss: 0.2540 - val_acc: 0.9298\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.93210\n",
      "Epoch 99/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.2164 - acc: 0.9416 - val_loss: 0.2558 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.93210\n",
      "Epoch 100/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.2150 - acc: 0.9401 - val_loss: 0.2467 - val_acc: 0.9311\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.93210\n",
      "Epoch 101/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.2167 - acc: 0.9419 - val_loss: 0.2562 - val_acc: 0.9281\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.93210\n",
      "Epoch 102/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2160 - acc: 0.9404 - val_loss: 0.2544 - val_acc: 0.9267\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.93210\n",
      "Epoch 103/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2119 - acc: 0.9422 - val_loss: 0.2512 - val_acc: 0.9293\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.93210\n",
      "Epoch 104/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2123 - acc: 0.9417 - val_loss: 0.2456 - val_acc: 0.9328\n",
      "\n",
      "Epoch 00104: val_acc improved from 0.93210 to 0.93280, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.104.h5\n",
      "Epoch 105/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2089 - acc: 0.9424 - val_loss: 0.2501 - val_acc: 0.9301\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.93280\n",
      "Epoch 106/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.2090 - acc: 0.9426 - val_loss: 0.2444 - val_acc: 0.9305\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.93280\n",
      "Epoch 107/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2090 - acc: 0.9422 - val_loss: 0.2433 - val_acc: 0.9318\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.93280\n",
      "Epoch 108/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2063 - acc: 0.9427 - val_loss: 0.2512 - val_acc: 0.9290\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.93280\n",
      "Epoch 109/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2041 - acc: 0.9432 - val_loss: 0.2499 - val_acc: 0.9291\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.93280\n",
      "Epoch 110/200\n",
      "Learning rate:  0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2068 - acc: 0.9431 - val_loss: 0.2469 - val_acc: 0.9318\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.93280\n",
      "Epoch 111/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2035 - acc: 0.9423 - val_loss: 0.2417 - val_acc: 0.9325\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.93280\n",
      "Epoch 112/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2013 - acc: 0.9452 - val_loss: 0.2406 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.93280\n",
      "Epoch 113/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2021 - acc: 0.9432 - val_loss: 0.2444 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.93280\n",
      "Epoch 114/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.2030 - acc: 0.9437 - val_loss: 0.2550 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.93280\n",
      "Epoch 115/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1964 - acc: 0.9445 - val_loss: 0.2435 - val_acc: 0.9320\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.93280\n",
      "Epoch 116/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.2029 - acc: 0.9436 - val_loss: 0.2438 - val_acc: 0.9317\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.93280\n",
      "Epoch 117/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1978 - acc: 0.9450 - val_loss: 0.2485 - val_acc: 0.9300\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.93280\n",
      "Epoch 118/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1978 - acc: 0.9447 - val_loss: 0.2436 - val_acc: 0.9318\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.93280\n",
      "Epoch 119/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1958 - acc: 0.9452 - val_loss: 0.2422 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.93280\n",
      "Epoch 120/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1953 - acc: 0.9448 - val_loss: 0.2383 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00120: val_acc improved from 0.93280 to 0.93400, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.120.h5\n",
      "Epoch 121/200\n",
      "Learning rate:  0.0004\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1974 - acc: 0.9431 - val_loss: 0.2426 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.93400\n",
      "Epoch 122/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1926 - acc: 0.9452 - val_loss: 0.2321 - val_acc: 0.9339\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.93400\n",
      "Epoch 123/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1854 - acc: 0.9483 - val_loss: 0.2318 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.93400\n",
      "Epoch 124/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1834 - acc: 0.9482 - val_loss: 0.2326 - val_acc: 0.9348\n",
      "\n",
      "Epoch 00124: val_acc improved from 0.93400 to 0.93480, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.124.h5\n",
      "Epoch 125/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1850 - acc: 0.9493 - val_loss: 0.2315 - val_acc: 0.9346\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.93480\n",
      "Epoch 126/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1872 - acc: 0.9479 - val_loss: 0.2322 - val_acc: 0.9353\n",
      "\n",
      "Epoch 00126: val_acc improved from 0.93480 to 0.93530, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.126.h5\n",
      "Epoch 127/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1847 - acc: 0.9479 - val_loss: 0.2327 - val_acc: 0.9347\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.93530\n",
      "Epoch 128/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1832 - acc: 0.9488 - val_loss: 0.2317 - val_acc: 0.9358\n",
      "\n",
      "Epoch 00128: val_acc improved from 0.93530 to 0.93580, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.128.h5\n",
      "Epoch 129/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1826 - acc: 0.9497 - val_loss: 0.2310 - val_acc: 0.9347\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.93580\n",
      "Epoch 130/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1830 - acc: 0.9491 - val_loss: 0.2310 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.93580\n",
      "Epoch 131/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1824 - acc: 0.9495 - val_loss: 0.2303 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.93580\n",
      "Epoch 132/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1845 - acc: 0.9479 - val_loss: 0.2320 - val_acc: 0.9353\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.93580\n",
      "Epoch 133/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1808 - acc: 0.9499 - val_loss: 0.2313 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00133: val_acc improved from 0.93580 to 0.93610, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.133.h5\n",
      "Epoch 134/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1838 - acc: 0.9491 - val_loss: 0.2304 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00134: val_acc improved from 0.93610 to 0.93670, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.134.h5\n",
      "Epoch 135/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1837 - acc: 0.9493 - val_loss: 0.2299 - val_acc: 0.9360\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.93670\n",
      "Epoch 136/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1826 - acc: 0.9487 - val_loss: 0.2300 - val_acc: 0.9370\n",
      "\n",
      "Epoch 00136: val_acc improved from 0.93670 to 0.93700, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.136.h5\n",
      "Epoch 137/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1817 - acc: 0.9503 - val_loss: 0.2301 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.93700\n",
      "Epoch 138/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1791 - acc: 0.9500 - val_loss: 0.2304 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.93700\n",
      "Epoch 139/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1832 - acc: 0.9490 - val_loss: 0.2294 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.93700\n",
      "Epoch 140/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 45s 97ms/step - loss: 0.1809 - acc: 0.9503 - val_loss: 0.2299 - val_acc: 0.9364\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.93700\n",
      "Epoch 141/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 46s 98ms/step - loss: 0.1814 - acc: 0.9497 - val_loss: 0.2314 - val_acc: 0.9356\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.93700\n",
      "Epoch 142/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 45s 95ms/step - loss: 0.1807 - acc: 0.9506 - val_loss: 0.2309 - val_acc: 0.9354\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.93700\n",
      "Epoch 143/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 45s 96ms/step - loss: 0.1777 - acc: 0.9497 - val_loss: 0.2318 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.93700\n",
      "Epoch 144/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 46s 99ms/step - loss: 0.1825 - acc: 0.9494 - val_loss: 0.2318 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.93700\n",
      "Epoch 145/200\n",
      "Learning rate:  4e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1803 - acc: 0.9500 - val_loss: 0.2312 - val_acc: 0.9364\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.93700\n",
      "Epoch 146/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1782 - acc: 0.9511 - val_loss: 0.2308 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.93700\n",
      "Epoch 147/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1823 - acc: 0.9492 - val_loss: 0.2288 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.93700\n",
      "Epoch 148/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 45s 96ms/step - loss: 0.1788 - acc: 0.9509 - val_loss: 0.2297 - val_acc: 0.9366\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.93700\n",
      "Epoch 149/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1781 - acc: 0.9509 - val_loss: 0.2295 - val_acc: 0.9371\n",
      "\n",
      "Epoch 00149: val_acc improved from 0.93700 to 0.93710, saving model to /home/ryan-desktop/Documents/research-2019/fashion-mnist/saved_models/Fuzzy-ResNet56v2_model.149.h5\n",
      "Epoch 150/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1797 - acc: 0.9505 - val_loss: 0.2290 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.93710\n",
      "Epoch 151/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 45s 97ms/step - loss: 0.1793 - acc: 0.9502 - val_loss: 0.2295 - val_acc: 0.9364\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.93710\n",
      "Epoch 152/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1815 - acc: 0.9489 - val_loss: 0.2285 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.93710\n",
      "Epoch 153/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1770 - acc: 0.9500 - val_loss: 0.2302 - val_acc: 0.9358\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.93710\n",
      "Epoch 154/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.1795 - acc: 0.9508 - val_loss: 0.2295 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.93710\n",
      "Epoch 155/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 46s 98ms/step - loss: 0.1824 - acc: 0.9496 - val_loss: 0.2299 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.93710\n",
      "Epoch 156/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1764 - acc: 0.9508 - val_loss: 0.2299 - val_acc: 0.9349\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.93710\n",
      "Epoch 157/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1775 - acc: 0.9507 - val_loss: 0.2300 - val_acc: 0.9360\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.93710\n",
      "Epoch 158/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1764 - acc: 0.9515 - val_loss: 0.2285 - val_acc: 0.9356\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.93710\n",
      "Epoch 159/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 46s 98ms/step - loss: 0.1795 - acc: 0.9503 - val_loss: 0.2293 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.93710\n",
      "Epoch 160/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1789 - acc: 0.9500 - val_loss: 0.2299 - val_acc: 0.9364\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.93710\n",
      "Epoch 161/200\n",
      "Learning rate:  4e-05\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1757 - acc: 0.9509 - val_loss: 0.2295 - val_acc: 0.9354\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.93710\n",
      "Epoch 162/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 45s 95ms/step - loss: 0.1781 - acc: 0.9504 - val_loss: 0.2297 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.93710\n",
      "Epoch 163/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 95ms/step - loss: 0.1771 - acc: 0.9507 - val_loss: 0.2289 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.93710\n",
      "Epoch 164/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 45s 95ms/step - loss: 0.1762 - acc: 0.9514 - val_loss: 0.2290 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.93710\n",
      "Epoch 165/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1779 - acc: 0.9502 - val_loss: 0.2290 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.93710\n",
      "Epoch 166/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1790 - acc: 0.9502 - val_loss: 0.2288 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.93710\n",
      "Epoch 167/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1761 - acc: 0.9514 - val_loss: 0.2289 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.93710\n",
      "Epoch 168/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1793 - acc: 0.9502 - val_loss: 0.2289 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.93710\n",
      "Epoch 169/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 45s 95ms/step - loss: 0.1759 - acc: 0.9507 - val_loss: 0.2286 - val_acc: 0.9362\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.93710\n",
      "Epoch 170/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 95ms/step - loss: 0.1742 - acc: 0.9519 - val_loss: 0.2286 - val_acc: 0.9360\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.93710\n",
      "Epoch 171/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1771 - acc: 0.9508 - val_loss: 0.2286 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.93710\n",
      "Epoch 172/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1750 - acc: 0.9513 - val_loss: 0.2290 - val_acc: 0.9360\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.93710\n",
      "Epoch 173/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 46s 99ms/step - loss: 0.1755 - acc: 0.9516 - val_loss: 0.2288 - val_acc: 0.9362\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.93710\n",
      "Epoch 174/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 45s 95ms/step - loss: 0.1781 - acc: 0.9499 - val_loss: 0.2290 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.93710\n",
      "Epoch 175/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 95ms/step - loss: 0.1756 - acc: 0.9517 - val_loss: 0.2290 - val_acc: 0.9362\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.93710\n",
      "Epoch 176/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1761 - acc: 0.9507 - val_loss: 0.2291 - val_acc: 0.9360\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.93710\n",
      "Epoch 177/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1775 - acc: 0.9519 - val_loss: 0.2289 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.93710\n",
      "Epoch 178/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 45s 95ms/step - loss: 0.1765 - acc: 0.9515 - val_loss: 0.2291 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.93710\n",
      "Epoch 179/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 95ms/step - loss: 0.1747 - acc: 0.9519 - val_loss: 0.2292 - val_acc: 0.9360\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.93710\n",
      "Epoch 180/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 93ms/step - loss: 0.1746 - acc: 0.9522 - val_loss: 0.2291 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.93710\n",
      "Epoch 181/200\n",
      "Learning rate:  4e-06\n",
      "468/468 [==============================] - 44s 95ms/step - loss: 0.1790 - acc: 0.9499 - val_loss: 0.2288 - val_acc: 0.9364\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.93710\n",
      "Epoch 182/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 45s 96ms/step - loss: 0.1763 - acc: 0.9509 - val_loss: 0.2287 - val_acc: 0.9364\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.93710\n",
      "Epoch 183/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1765 - acc: 0.9503 - val_loss: 0.2289 - val_acc: 0.9364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00183: val_acc did not improve from 0.93710\n",
      "Epoch 184/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 44s 95ms/step - loss: 0.1756 - acc: 0.9511 - val_loss: 0.2288 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.93710\n",
      "Epoch 185/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1752 - acc: 0.9511 - val_loss: 0.2288 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.93710\n",
      "Epoch 186/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 45s 95ms/step - loss: 0.1760 - acc: 0.9513 - val_loss: 0.2289 - val_acc: 0.9369\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.93710\n",
      "Epoch 187/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1744 - acc: 0.9526 - val_loss: 0.2288 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.93710\n",
      "Epoch 188/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 45s 95ms/step - loss: 0.1778 - acc: 0.9508 - val_loss: 0.2291 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.93710\n",
      "Epoch 189/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1759 - acc: 0.9517 - val_loss: 0.2288 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.93710\n",
      "Epoch 190/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 45s 96ms/step - loss: 0.1780 - acc: 0.9503 - val_loss: 0.2289 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.93710\n",
      "Epoch 191/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 45s 97ms/step - loss: 0.1755 - acc: 0.9514 - val_loss: 0.2290 - val_acc: 0.9366\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.93710\n",
      "Epoch 192/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1764 - acc: 0.9506 - val_loss: 0.2289 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.93710\n",
      "Epoch 193/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 46s 98ms/step - loss: 0.1763 - acc: 0.9512 - val_loss: 0.2290 - val_acc: 0.9368\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.93710\n",
      "Epoch 194/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 46s 97ms/step - loss: 0.1768 - acc: 0.9512 - val_loss: 0.2289 - val_acc: 0.9364\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.93710\n",
      "Epoch 195/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1765 - acc: 0.9515 - val_loss: 0.2289 - val_acc: 0.9366\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.93710\n",
      "Epoch 196/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 45s 96ms/step - loss: 0.1755 - acc: 0.9522 - val_loss: 0.2292 - val_acc: 0.9362\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.93710\n",
      "Epoch 197/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1766 - acc: 0.9509 - val_loss: 0.2289 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.93710\n",
      "Epoch 198/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 45s 96ms/step - loss: 0.1754 - acc: 0.9513 - val_loss: 0.2289 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.93710\n",
      "Epoch 199/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 44s 95ms/step - loss: 0.1754 - acc: 0.9525 - val_loss: 0.2289 - val_acc: 0.9369\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.93710\n",
      "Epoch 200/200\n",
      "Learning rate:  2e-06\n",
      "468/468 [==============================] - 44s 94ms/step - loss: 0.1745 - acc: 0.9521 - val_loss: 0.2287 - val_acc: 0.9366\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.93710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5a701cc898>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    # set input mean to 0 over the dataset\n",
    "    featurewise_center=False,\n",
    "    # set each sample mean to 0\n",
    "    samplewise_center=False,\n",
    "    # divide inputs by std of dataset\n",
    "    featurewise_std_normalization=False,\n",
    "    # divide each input by its std\n",
    "    samplewise_std_normalization=False,\n",
    "    # apply ZCA whitening\n",
    "    zca_whitening=False,\n",
    "    # epsilon for ZCA whitening\n",
    "    zca_epsilon=1e-06,\n",
    "    # randomly rotate images in the range (deg 0 to 180)\n",
    "    rotation_range=0,\n",
    "    # randomly shift images horizontally\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically\n",
    "    height_shift_range=0.1,\n",
    "    # set range for random shear\n",
    "    shear_range=0.,\n",
    "    # set range for random zoom\n",
    "    zoom_range=0.,\n",
    "    # set range for random channel shifts\n",
    "    channel_shift_range=0.,\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    # value used for fill_mode = \"constant\"\n",
    "    cval=0.,\n",
    "    # randomly flip images\n",
    "    horizontal_flip=True,\n",
    "    # randomly flip images\n",
    "    vertical_flip=False,\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "model.fit_generator(\n",
    "    datagen.flow(x_train, y_train, batch_size=128),\n",
    "    steps_per_epoch=60000 // 128,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=200, \n",
    "    verbose=1, \n",
    "    workers=4,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-961de6383206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1440 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(5, 1, figsize=(12, 20), squeeze=True)\n",
    "axes[0].set_title(\"Loss\")\n",
    "axes[0].plot(history.history[\"loss\"], c=\"b\")\n",
    "axes[0].plot(history.history[\"val_loss\"], c=\"r\")\n",
    "\n",
    "axes[1].set_title(\"Log Loss\")\n",
    "axes[1].plot(np.log(history.history[\"loss\"]), c=\"b\")\n",
    "axes[1].plot(np.log(history.history[\"val_loss\"]), c=\"r\")\n",
    "\n",
    "axes[2].set_title(\"Accuracy\")\n",
    "axes[2].plot(history.history[\"acc\"], c=\"b\")\n",
    "axes[2].plot(history.history[\"val_acc\"], c=\"r\")\n",
    "\n",
    "axes[3].set_title(\"Accuracy\")\n",
    "axes[3].set_ylim((0.90, 1.05))\n",
    "axes[3].plot(history.history[\"acc\"], c=\"b\")\n",
    "axes[3].plot(history.history[\"val_acc\"], c=\"r\")\n",
    "\n",
    "axes[4].set_title(\"Log Learning Rate\")\n",
    "axes[4].plot(np.log(history.history[\"lr\"]), c=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
